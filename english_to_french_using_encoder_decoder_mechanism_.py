# -*- coding: utf-8 -*-
"""English to French using encoder decoder mechanism .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-F7CU1E_NlHislc6nTMOdsEQGnw0Txbg
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded=files.upload()

# Load the dataset
data = pd.read_csv('english_french.csv')

# Data Cleaning
def clean_text(text):
    text = text.lower()
    text = text.replace("\n", " ")
    text = text.replace("\r", " ")
    return text

data['English'] = data['English'].apply(clean_text)
data['French'] = data['French'].apply(clean_text)

# Tokenization and Sequences
def preprocess_data(texts, num_words):
    tokenizer = Tokenizer(num_words=num_words, filters='', lower=True)
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    return tokenizer, sequences

# Hyperparameters
num_words = 10000
max_len = 20

# English
eng_tokenizer, eng_sequences = preprocess_data(data['English'], num_words)
eng_word_index = eng_tokenizer.word_index
eng_padded = pad_sequences(eng_sequences, maxlen=max_len, padding='post')

# French
fr_tokenizer, fr_sequences = preprocess_data(data['French'], num_words)
fr_word_index = fr_tokenizer.word_index
fr_padded = pad_sequences(fr_sequences, maxlen=max_len, padding='post')

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(eng_padded, fr_padded, test_size=0.2, random_state=42)

# Encoder-Decoder Model
embedding_dim = 256
units = 256

# Encoder
encoder_inputs = Input(shape=(max_len,))  # Input shape (batch_size, max_len)
encoder_embedding = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len)(encoder_inputs)  # Add Embedding layer
encoder_lstm, state_h, state_c = LSTM(units, return_state=True)(encoder_embedding)  # Output shape is now 3D
encoder_states = [state_h, state_c]


# Decoder
decoder_inputs = Input(shape=(max_len,))  # Input shape (batch_size, max_len)
decoder_embedding = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embedding_dim)(decoder_inputs)  # Add Embedding layer
decoder_lstm, _, _ = LSTM(units, return_sequences=True, return_state=True)(decoder_embedding, initial_state=encoder_states)  # Ensure the LSTM receives 3D input
decoder_dense = Dense(num_words, activation='softmax')
decoder_outputs = decoder_dense(decoder_lstm)


# Compile Model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train Model
y_train = np.expand_dims(y_train, axis=-1)  # Reshape target data
y_test = np.expand_dims(y_test, axis=-1)

history = model.fit([X_train, X_train], y_train, epochs=20, batch_size=64, validation_data=([X_test, X_test], y_test))



# Save the Model
model.save('english_to_french_translation_model.h5')

# Translation Function
def translate_sentence(sentence, model, tokenizer_src, tokenizer_tgt):
    sequence = tokenizer_src.texts_to_sequences([sentence])
    sequence_padded = pad_sequences(sequence, maxlen=max_len, padding='post')
    pred_sequence = model.predict([sequence_padded, sequence_padded])
    pred_indices = np.argmax(pred_sequence, axis=-1)
    pred_words = [tokenizer_tgt.index_word.get(idx, '') for idx in pred_indices[0]]
    return ' '.join(pred_words)

# Example Translation
english_sentence = "How are you?"
translation = translate_sentence(english_sentence, model, eng_tokenizer, fr_tokenizer)
print("Translated Sentence:", translation)